{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共有12000多张图片，每张图片image对应一个句子caption\n",
    "\n",
    "json文件的键值对就是图片名+对应的句子，train有10000张，test有2000张\n",
    "\n",
    "第一步就是先构建词表，每个词对应的数字编码，保存下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9affdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def create_vocab(dataset='deepfashion-multimodal',min_word_count=5):\n",
    "\n",
    "    test_json_path='./data/%s/test_captions.json' % dataset\n",
    "    image_folder='./data/%s/images' % dataset\n",
    "    output_folder='./data/%s' % dataset\n",
    "\n",
    "    with open(test_json_path, 'r') as j:\n",
    "        data = json.load(j)\n",
    "    vocab = Counter()\n",
    "    for k in data.keys():\n",
    "        caption=data[k]\n",
    "        word_lists=caption.split()\n",
    "        vocab.update(word_lists)\n",
    "    \n",
    "    print(len(vocab))\n",
    "    \n",
    "    # 创建词典，增加占位标识符<pad>、未登录词标识符<unk>、句子首尾标识符<start>和<end>\n",
    "    #这个vocab是给每个词一个value，用来编码，如：dog：31，编码方式仅仅是词数组的顺序\n",
    "    #去掉出现频率小于5的词\n",
    "    words = [w for w in vocab.keys() if vocab[w] > min_word_count]\n",
    "    vocab = {k: v + 1 for v, k in enumerate(words)}\n",
    "    vocab['<pad>'] = 0                              \n",
    "    vocab['<unk>'] = len(vocab)\n",
    "    vocab['<start>'] = len(vocab)\n",
    "    vocab['<end>'] = len(vocab)\n",
    "    print(len(vocab))\n",
    "\n",
    "    # 存储词典\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as fw:\n",
    "        json.dump(vocab, fw)\n",
    "\n",
    "#create_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集类，继承自Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch数据类，用于PyTorch DataLoader来按批次产生数据\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_path, captions_path ,vocab_path, max_len=50, transform=None):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            images_path：图片文件夹路径\n",
    "            captions_path：描述句子json文件路径\n",
    "            vocab_path：词典json文件路径\n",
    "            max_len：文本描述包含的最大单词数\n",
    "            transform: 图像预处理方法\n",
    "        \"\"\"\n",
    "        self.data=[]\n",
    "        self.max_len = max_len\n",
    "        self.images_path=images_path\n",
    "        # 载入数据集\n",
    "        with open(captions_path, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        for index, key in enumerate(self.captions):\n",
    "            value=self.captions[key].split()           #['a','women','is','wearing',...]\n",
    "            #如果描述的句子太长，自动截断\n",
    "            if(len(value)>max_len):\n",
    "                value=value[:max_len]\n",
    "            value=[self.vocab['<start>']]+[self.vocab.get(word, self.vocab['<unk>']) for word in value]+[self.vocab['<end>']]\n",
    "            self.data.append([key,value])\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        image_file_path=os.path.join(self.images_path, self.data[i][0])\n",
    "        img = Image.open(image_file_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img_tensor = self.transform(img)\n",
    "\n",
    "        caplen = len(self.data[i][1])\n",
    "        #在dataloader中，拿到的最长长度是maxlen+2，因为加了起始符和终止符\n",
    "        #self.max_len + 3 - caplen是因为maxlen+2减去caplen-1\n",
    "        target_caption=torch.LongTensor(self.data[i][1][1:]+ [self.vocab['<pad>']] * (self.max_len + 3 - caplen))\n",
    "        input_caption=torch.LongTensor(self.data[i][1][:-1]+ [self.vocab['<pad>']] * (self.max_len + 3 - caplen))\n",
    "        \n",
    "        tgt_padding_mask = torch.ones([self.max_len+2, ])\n",
    "        tgt_padding_mask[:caplen-1] = 0.0\n",
    "        tgt_padding_mask = tgt_padding_mask.bool()\n",
    "        \n",
    "        return img_tensor, input_caption, target_caption,tgt_padding_mask,image_file_path\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "train_tx = transforms.Compose([\n",
    "        #transforms.Resize((1100,750)),\n",
    "        transforms.Resize((1000,700)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "test_tx = transforms.Compose([\n",
    "        #transforms.Resize((1100,750)),\n",
    "        transforms.Resize((1000,700)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像编码器\n",
    "\n",
    "直接使用pytorch训练好的res101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
    "import json\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"残差块\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"本体加上两个线性变化\"\"\"\n",
    "        skip_connection = x\n",
    "        x = self.block(x)\n",
    "        x = skip_connection + x\n",
    "        return x\n",
    "\n",
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"原始的位置编码\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model, p_dropout):\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        #[[0],[1],[2],[3],...,[9]],shape=(10,1)\n",
    "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
    "        #[[0,1,2,...,99]],shape=(1,100)\n",
    "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
    "        angles = token_positions / (10000 ** ((2 * dim_positions) / d_model))\n",
    "        #angles.shape=(10,100)\n",
    "\n",
    "        encodings = torch.zeros(1, seq_len, d_model)\n",
    "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])    #偶数位置用cos编码\n",
    "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])  #奇数位置用sin编码\n",
    "        encodings.requires_grad = False\n",
    "        self.register_buffer(\"positional_encodings\", encodings)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encodings\n",
    "        x = self.dropout(x)\n",
    "        return x   \n",
    " \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab:dict,word_dim:int=512,depth:int=6,att_head:int=8,\n",
    "                 ff_dim:int=1024,drop:int=0.5,max_len:int=50,img_feat_channels:int=2048\n",
    "                 ) -> None:\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.vocab_size=len(vocab)\n",
    "\n",
    "        #将每个词映射到一个高维度表示\n",
    "        self.embedding_layer=nn.Embedding(self.vocab_size,word_dim)\n",
    "        self.res_block = ResidualBlock(word_dim)\n",
    "        #给每个词加上位置编码\n",
    "        self.positional_encodings = PositionalEncodings(max_len+2, word_dim, drop)\n",
    "\n",
    "        #将图片的维度降到和词的维度相同\n",
    "        self.entry_mapping_img = nn.Linear(img_feat_channels, word_dim)\n",
    "        \n",
    "        transformer_decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=word_dim,\n",
    "            nhead=att_head,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=drop,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(transformer_decoder_layer, depth)     #depth:transformer中有一个DecoderLayer\n",
    "        \n",
    "        #将高维word表示映射回字典大小维度，最大值索引就是对应的词\n",
    "        #为何最后的线性映射只用一个linear?\n",
    "        #因为transformer每个decoderLayer最后自带forwardfeed，也就是说在最终输出前已经经过线性层升维+激活+dropout+线性层降维\n",
    "        self.classifier = nn.Linear(word_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, image_features, tgt_padding_mask=None, tgt_mask=None):\n",
    "        '''\n",
    "        解码器工作流程：\n",
    "        1.接受编码器给的(batch, H*W, C)的张量后,先把C降维到word_dim,用来做cross-attention,再做一次激活\n",
    "        2.x是训练时的输入,也就是ground_truth,shape=(batch,max_len+2),不过第0个元素是起始符,最后一个元素是pad\n",
    "        3.用embeddingLayer将每个词映射到高维度中,也就是(batch,max_len+2,word_dim)\n",
    "        4.将这个映射后的输入经过激活+残差连接\n",
    "        5.将处理过的输入再加入位置编码\n",
    "        6.有了图片的表示和输入的表示就可以直接丢给pytorch定义好的transformerDecoder\n",
    "        7.还要把padding_mask和tgt_mask穿进去\n",
    "        8.经过decoder后的输出仍是(batch,maxlen+2,word_dim),用一个线性变换将word_dim映射到vocab_size维,最大的值对应的索引就是预测词\n",
    "        '''\n",
    "        #输入进来的image_feature已经被拉平了，是(batch,HW,C)\n",
    "        image_features = self.entry_mapping_img(image_features)\n",
    "        image_features = F.leaky_relu(image_features)\n",
    "\n",
    "        x = self.embedding_layer(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.res_block(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.positional_encodings(x)\n",
    "\n",
    "        x = self.decoder(\n",
    "            tgt=x,\n",
    "            memory=image_features,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个模型由编码器和解码器组成，额外定义了一个函数用于接受一张图片，输出一个描述的句子\n",
    "\n",
    "这个函数是通过for循环句子的最大长度实现的，考虑模型训练时是使用ground-truth，也就是说后面的看到前面的必须得是正确的\n",
    "\n",
    "如果一次性将整个啥都不是的句子丢给transformer，预测的结果可能不好\n",
    "\n",
    "因此通过for循环，先预测第一个单词，然后再到第二个单词在看第一个单词的基础上预测，后面的以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet101,ResNet101_Weights\n",
    "\n",
    "class Image_Caption_Generator(nn.Module):\n",
    "    def __init__(self,vocab_path:str,max_len:int=50,word_dim:int=512) -> None:\n",
    "        super(Image_Caption_Generator,self).__init__()\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab=json.load(f)\n",
    "        self.vocab_size=len(self.vocab)\n",
    "        self.max_len=max_len\n",
    "\n",
    "        encoder=resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        encoder=torch.nn.Sequential(*list(encoder.children())[:-2])\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        #图像编码器\n",
    "        self.encoder=encoder\n",
    "        #res101出来的特征图channels是2048\n",
    "\n",
    "        #图像解码器\n",
    "        self.decoder=Decoder(self.vocab,word_dim=word_dim,max_len=self.max_len)\n",
    "\n",
    "    def forward(self,imgs,input_captions,tgt_padding_mask):\n",
    "        with torch.no_grad():   #编码的前向传播不需要计算梯度\n",
    "            output=self.encoder(imgs)\n",
    "            #将图像的宽高拉成一条直线\n",
    "            img_features = output.view(output.size(0), output.size(1), -1)\n",
    "            #转换位置，让最后一维表示每个像素的维度\n",
    "            img_features = img_features.permute(0, 2, 1)\n",
    "            img_features = img_features.detach()\n",
    "\n",
    "        tgt_mask=nn.Transformer.generate_square_subsequent_mask(self.max_len+2).to('cuda')\n",
    "        preds=self.decoder(input_captions,img_features,tgt_padding_mask=tgt_padding_mask,tgt_mask=tgt_mask)\n",
    "        return preds    #(batch,max_len+2,vocab_size)\n",
    "    \n",
    "    def generate_caption(self,image):\n",
    "        assert len(image.shape)==4,'输入的图片张量应该是4维的'\n",
    "        assert image.size(0)==1,'每次输进来的图片只能是一张'\n",
    "        device=image.device\n",
    "        output=[]\n",
    "\n",
    "        image_feature=self.encoder(image)\n",
    "        image_feature = image_feature.view(image_feature.size(0), image_feature.size(1), -1)\n",
    "        image_feature = image_feature.permute(0, 2, 1)\n",
    "        image_feature = image_feature.detach()\n",
    "\n",
    "        input = torch.Tensor([self.vocab['<start>']] + [self.vocab['<pad>']] * (self.max_len+1)).to(device).long()\n",
    "        padd_mask = torch.Tensor([True] * (self.max_len+2)).to(device).bool().unsqueeze(0)\n",
    "        input=input.unsqueeze(0)\n",
    "\n",
    "        tgt_mask=nn.Transformer.generate_square_subsequent_mask(self.max_len+2).to(device)\n",
    "        for i in range(self.max_len+1):\n",
    "            padd_mask[0,i]=False                #每次\n",
    "            pred=self.decoder(input,image_feature,padd_mask,tgt_mask=tgt_mask)  #输出是(1,max_len+2,vocab_size)\n",
    "            word_num=pred[0,i].topk(1)[1].item()      #取得当前位置最有可能的词的代号\n",
    "            if(word_num==self.vocab['<end>']):    #如果是结束符，那就退出循环\n",
    "                break\n",
    "            output.append(word_num)\n",
    "            input[0,i+1]=word_num\n",
    "        #将数字转成词\n",
    "        output=[list(self.vocab.keys())[list(self.vocab.values()).index(n)] for n in output]\n",
    "        return ' '.join(output)\n",
    "\n",
    "def cut_caption(caption):\n",
    "    '''\n",
    "    因为网络训练的数据是减掉了max_Len后面的句子,因此网络经过训练后输出的也是“减掉”的句子\n",
    "    为了美观，手动将最后一个句号后面的东西丢掉\n",
    "    '''\n",
    "    cap_len=len(caption)\n",
    "    for i in range(cap_len-1,-1,-1):\n",
    "        if(caption[i]=='.'):\n",
    "            return caption[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace \n",
    "config = Namespace(\n",
    "    max_len = 40,           #描述最大长度\n",
    "    batch_size = 16,        #训练每次传进去图片的数量\n",
    "    image_code_dim = 2048,  #图片经过encoder，在decoder之前的维度\n",
    "    word_dim = 512,         #词经过embedding层后的维度，也是图片要降成的维度\n",
    "    decoderLayer_num=6,     #transformer中decoderLayer的个数\n",
    "    head_num=8,             #多头机制的头数\n",
    "    feedforward_dim=1024,   #transformer中前馈网络神经元的维度\n",
    "    drop=0.5,               #前馈网络drop率\n",
    "    learning_rate = 0.0001, #学习率\n",
    "    num_epochs = 10,        #训练的epoch数\n",
    "    grad_clip = 5.0,        #要截断的梯度参数\n",
    "    evaluate_step=200,      #训练时个epoch多少轮在测试集上评估一次\n",
    ")\n",
    "IMAGE_FOLDER='./data/deepfashion-multimodal/images'\n",
    "TRAIN_CAPTIONS_PATH='./data/deepfashion-multimodal/train_captions.json'\n",
    "TEST_CAPTIONS_PATH='./data/deepfashion-multimodal/test_captions.json'\n",
    "VOCAB_PATH='./data/deepfashion-multimodal/vocab.json'\n",
    "CHECKPOINT_PATH='./data/checkpoint.pt'\n",
    "train_data=ImageTextDataset(IMAGE_FOLDER,TRAIN_CAPTIONS_PATH,VOCAB_PATH,max_len=config.max_len,transform=train_tx)\n",
    "test_data=ImageTextDataset(IMAGE_FOLDER,TEST_CAPTIONS_PATH,VOCAB_PATH,max_len=config.max_len,transform=test_tx)\n",
    "train_loader=DataLoader(train_data,batch_size=16,shuffle=True,pin_memory=True)\n",
    "test_loader=DataLoader(test_data,batch_size=4,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5790505088385673\n",
      "1.5790505088385673\n",
      "1.9994251538919205\n",
      "3.578475662730488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(total_score)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_score\n\u001b[1;32m---> 58\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     40\u001b[0m img\u001b[38;5;241m=\u001b[39mimgs[b]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m img\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 42\u001b[0m preds\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m caption\u001b[38;5;241m=\u001b[39mcut_caption(preds)\n\u001b[0;32m     44\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(caption)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mImage_Caption_Generator.generate_caption\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     52\u001b[0m     padd_mask[\u001b[38;5;241m0\u001b[39m,i]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m                \u001b[38;5;66;03m#每次\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mimage_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadd_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#输出是(1,max_len+2,vocab_size)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     word_num\u001b[38;5;241m=\u001b[39mpred[\u001b[38;5;241m0\u001b[39m,i]\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()      \u001b[38;5;66;03m#取得当前位置最有可能的词的代号\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(word_num\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m]):    \u001b[38;5;66;03m#如果是结束符，那就退出循环\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, image_features, tgt_padding_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[0;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encodings(x)\n\u001b[1;32m--> 103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py:291\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    288\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 291\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py:578\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    576\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m    577\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask))\n\u001b[1;32m--> 578\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 602\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout3(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate import meteor_score\n",
    "unloader=transforms.ToPILImage()\n",
    "\n",
    "def show(img_path,model):\n",
    "    ''' 给任意一张图片的路径,输出图像和对应描述 '''\n",
    "    img=Image.open(img_path)\n",
    "    img_tensor=test_tx(img)\n",
    "    img_tensor=img_tensor.unsqueeze(0)\n",
    "    preds=model.generate_caption(img)\n",
    "    caption=cut_caption(preds)\n",
    "    plt.imshow(img)\n",
    "    plt.title(caption)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_meteor_score(reference_texts,generated_texts,batch_size):\n",
    "    sum_score=0\n",
    "    for i in range(batch_size):\n",
    "        r=reference_texts[i]\n",
    "        g=generated_texts[i]\n",
    "        ''' 考虑到g是截断的,因此提供r的全长和截断长度'''\n",
    "        length=len(g)\n",
    "        meteor = meteor_score.meteor_score([r[:length].split(),r.split()], g.split())\n",
    "        sum_score+=meteor\n",
    "    return sum_score\n",
    "\n",
    "def test(model:Image_Caption_Generator or None=None):\n",
    "    total_score=0\n",
    "    if(model is None):\n",
    "        model=Image_Caption_Generator(VOCAB_PATH,config.max_len)\n",
    "        model.decoder.eval()\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    device=next(model.parameters()).device\n",
    "    for i,(imgs,input_captions,target_captions,tgt_padding_mask,paths) in enumerate(test_loader):\n",
    "        batch_size=imgs.size(0)\n",
    "        outputs=[]\n",
    "        #预测出结果，并加入输出列表\n",
    "        for b in range(batch_size):\n",
    "            img=imgs[b].unsqueeze(0)\n",
    "            img=img.to(device)\n",
    "            preds=model.generate_caption(img)\n",
    "            caption=cut_caption(preds)\n",
    "            outputs.append(caption)\n",
    "\n",
    "        paths=[path.replace(IMAGE_FOLDER+'\\\\','') for path in paths]\n",
    "\n",
    "        #加载训练集字典，找到图片对应的描述\n",
    "        with open(TEST_CAPTIONS_PATH,'r') as f:\n",
    "            test_dict=json.load(f)\n",
    "        reference_texts=[test_dict[path] for path in paths]\n",
    "        #计算预测描述和目标描述得分\n",
    "        score = calculate_meteor_score(reference_texts,outputs,batch_size)\n",
    "        total_score+=score\n",
    "    return total_score\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n",
      "epoch 0, step 100: loss=3.03\n",
      "epoch 0, step 200: loss=2.16\n",
      "Validation@epoch, 0, step, 200, meteor=0.75\n",
      "epoch 0, step 300: loss=1.79\n",
      "epoch 0, step 400: loss=1.63\n",
      "Validation@epoch, 0, step, 400, meteor=0.65\n",
      "epoch 0, step 500: loss=1.62\n",
      "epoch 0, step 600: loss=1.50\n",
      "Validation@epoch, 0, step, 600, meteor=0.91\n",
      "epoch 1, step 100: loss=1.42\n",
      "epoch 1, step 200: loss=1.39\n",
      "Validation@epoch, 1, step, 200, meteor=1.22\n",
      "epoch 1, step 300: loss=1.35\n",
      "epoch 1, step 400: loss=1.35\n",
      "Validation@epoch, 1, step, 400, meteor=1.17\n",
      "epoch 1, step 500: loss=1.35\n",
      "epoch 1, step 600: loss=1.34\n",
      "Validation@epoch, 1, step, 600, meteor=1.14\n",
      "epoch 2, step 100: loss=1.31\n",
      "epoch 2, step 200: loss=1.31\n",
      "Validation@epoch, 2, step, 200, meteor=1.08\n",
      "epoch 2, step 300: loss=1.30\n"
     ]
    }
   ],
   "source": [
    "def train(epochs:int=5,lr:int=0.0001,ckpt_path:str or None=None):\n",
    "    #设定设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    #初始化所有东西\n",
    "    loss_func=nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    model=Image_Caption_Generator(VOCAB_PATH,config.max_len)\n",
    "    if(ckpt_path is not None):\n",
    "        model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.encoder.eval()\n",
    "    model.decoder.train()\n",
    "    model.to(device)\n",
    "    optim=torch.optim.Adam(model.decoder.parameters(),lr=lr)\n",
    "\n",
    "    best_score=0.0\n",
    "    print(\"开始训练\")\n",
    "    fw = open('./data/log.txt', 'w')\n",
    "\n",
    "    #return\n",
    "    for e in range(epochs):\n",
    "        loss=0\n",
    "        for i, (imgs,input_captions,target_captions,tgt_padding_mask,_) in enumerate(train_loader):\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            #将数据全部移动到gpu上\n",
    "            imgs=imgs.to(device)\n",
    "            input_captions=input_captions.to(device)\n",
    "            target_captions=target_captions.to(device)\n",
    "            tgt_padding_mask=tgt_padding_mask.to(device)\n",
    "            \n",
    "\n",
    "            #预测出结果\n",
    "            preds=model(imgs,input_captions,tgt_padding_mask)\n",
    "            \n",
    "            tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n",
    "            preds = preds[tgt_padding_mask]         \n",
    "            #从preds中选出不为pad的位置，顺便shape从3维降成了2维，(？,vocab_size)问号取决于mask\n",
    "            #同理从目标句子选出不为pad的位置，shape从2维降成1维,(?)，每个值应该在区间[0,vocab_size]之间\n",
    "            target_captions= target_captions[tgt_padding_mask]\n",
    "\n",
    "            loss = loss_func(preds,target_captions)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.decoder.parameters(), config.grad_clip)\n",
    "            optim.step()\n",
    "            #每一百次记录一次损失\n",
    "            if(i%100==99):\n",
    "                print('epoch %d, step %d: loss=%.2f' % (e, i+1, loss.cpu()))\n",
    "                fw.write('epoch %d, step %d: loss=%.2f \\n' % (e, i+1, loss.cpu()))\n",
    "                fw.flush()\n",
    "            #每若干次在测试集上进行计算meteor指标并保存\n",
    "            if(i%config.evaluate_step==(config.evaluate_step-1)):\n",
    "                #torch.cuda.empty_cache()\n",
    "                model.decoder.eval()\n",
    "                this_score=test(model)\n",
    "                model.decoder.train()\n",
    "                if(best_score<this_score):\n",
    "                    best_score=this_score\n",
    "                    torch.save(model.state_dict(),CHECKPOINT_PATH)\n",
    "                fw.write('Validation@epoch, %d, step, %d, meteor=%.2f\\n' % \n",
    "                  (e, i+1, this_score))\n",
    "                fw.flush()\n",
    "                print('Validation@epoch, %d, step, %d, meteor=%.2f' % \n",
    "                    (e, i+1, this_score))\n",
    "        \n",
    "train()\n",
    "#train(3,ckpt_path='./data/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False],\n",
      "        [ True,  True]])\n",
      "tensor([1, 2, 1])\n",
      "tensor(1.0023)\n"
     ]
    }
   ],
   "source": [
    "b=torch.logical_not(torch.tensor([[False,True],[False,False]]))\n",
    "a=torch.tensor([[[0,0.1,0.2],[1,1.3,1.5]],[[2,2.4,2.6],[3,3.7,3.9]]])\n",
    "c=torch.tensor([[1,0],[2,1]])\n",
    "print(b)\n",
    "print(c[b])\n",
    "a[b]\n",
    "#交叉熵损失函数可以自动将模型预测的vocab_Size维词向量和词对应数字编码做损失\n",
    "#即pred=(batch,class_num),tgt=(batch),tgt每个元素的取值必须在0~class_num-1之间\n",
    "#但交叉熵也比较反直觉，对于3维及以上的预测结果(batch,len,dim)和(batch,len),如果要丢进交叉熵里，得先将len和dim调换位置\n",
    "loss_fcn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "l=loss_fcn(a[b],c[b])\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
